{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python mixed_training.py -d Pictures\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Nadam\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = '/Users/artemsmirnov/Desktop/for_my_data/Pictures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Y1\", \"X3_dummy\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\", \"X14_dummy\", \"logX15\", \"logX16\", \"X17\", \"X18\", \"X19\", \"logX20\", \"logX21\", \"X22\", \"X23_dummy\", \"X24_dummy\", \"X25\", \"X26\", \"X27\", \"X28\", \"X29\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/artemsmirnov/Desktop/Pictures/alpha_alpha.txt', sep=\" \", header=None, names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have to extract our satellite imagery and make an array of the pictures. Every picture contains two parts. \n",
    "#The left one is the beginning of summer, the right side is the end of the summer.\n",
    "\n",
    "def load_field_images(df, inputPath):\n",
    "    # initialize our images array (i.e., the field images themselves)\n",
    "    images = []\n",
    "\n",
    "    # loop over the indexes of the fields\n",
    "    for i in df.index.values:\n",
    "         # find the two images for the field and sort the file paths,\n",
    "        # ensuring the two are always in the *same order*\n",
    "        basePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
    "        fieldPaths = sorted(list(glob.glob(basePath)))\n",
    "\n",
    "        # initialize our list of input images along with the output image\n",
    "        # after *combining* the two input images\n",
    "        inputImages = []\n",
    "        outputImage = np.zeros((64, 128, 3), dtype=\"uint8\")\n",
    "\n",
    "\n",
    "        # loop over the input field paths\n",
    "        for fieldPath in fieldPaths:\n",
    "            # load the input image, resize it to be 64 64, and then\n",
    "            # update the list of input images\n",
    "            # each photo consists of 64 pixels\n",
    "            image = cv2.imread(fieldPath)\n",
    "            image = cv2.resize(image, (64, 64))\n",
    "            inputImages.append(image)\n",
    "\n",
    "\n",
    "        # tile the two input images in the output image such the first\n",
    "        # image goes in the right corner, the second image in the\n",
    "        # left corner\n",
    "        \n",
    "        outputImage[0:64, 0:64] = inputImages[0]\n",
    "        outputImage[0:64, 64:128] = inputImages[1]\n",
    "\n",
    "        # add the tiled image to our set of images the network will be\n",
    "        # trained on\n",
    "        images.append(outputImage)\n",
    "\n",
    "    # return our set of images\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_field_images(df, inputPath)\n",
    "images = images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = train_test_split(df, images, test_size=0.25, random_state=42)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
    "trainY = trainAttrX[\"Y1\"]\n",
    "testY = testAttrX[\"Y1\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zca_whitening=True)\n",
    "# fit parameters from data\n",
    "datagen.fit(trainImagesX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_field_attributes(df, train, test):\n",
    "    # initialize the column names of the continuous data\n",
    "    continuous = [\"X3_dummy\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\", \"X14_dummy\", \"logX15\", \"logX16\", \"X17\", \"X18\", \"X19\", \"logX20\", \"logX21\", \"X22\", \"X23_dummy\", \"X24_dummy\", \"X25\", \"X26\", \"X27\", \"X28\", \"X29\"]\n",
    "\n",
    "    # performin min-max scaling each continuous feature column to\n",
    "    # the range [0, 1]\n",
    "    cs = StandardScaler()\n",
    "    trainContinuous = cs.fit_transform(train[continuous])\n",
    "    testContinuous = cs.transform(test[continuous])\n",
    "\n",
    "    # one-hot encode the zip code categorical data (by definition of\n",
    "    # one-hot encoing, all output features are now in the range [0, 1])\n",
    "    #zipBinarizer = LabelBinarizer().fit(df[[\"No\"]])\n",
    "    #trainCategorical = zipBinarizer.transform(train[[\"No\"]])\n",
    "    #testCategorical = zipBinarizer.transform(test[[\"No\"]])\n",
    "\n",
    "    # construct our training and testing data points by concatenating\n",
    "    # the categorical features with the continuous features\n",
    "    trainX = np.hstack([trainContinuous])\n",
    "    testX = np.hstack([testContinuous])\n",
    "\n",
    "    # return the concatenated training and testing data\n",
    "    return (trainX, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainAttrX, testAttrX) = process_field_attributes(df, trainAttrX, testAttrX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth, filters=(3, 32, 64), regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "        # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "\n",
    "        # CONV => RELU => BN => POOL\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(3)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    #if regress:\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(70, input_dim=dim, activation=\"sigmoid\"))\n",
    "    model.add(Dense(25, activation=\"sigmoid\"))\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "   # if regress:\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the MLP and CNN models\n",
    "cnn = create_cnn(128, 64, 3, regress=False)\n",
    "mlp = create_mlp(trainAttrX.shape[1], regress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAttrX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateFactor = Input(tensor = K.variable([0.3]))\n",
    "model = Model(inputs=[mlp.input, cnn.input, gateFactor], outputs=x)\n",
    "opt = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "#opt = SGD(lr=0.001, momentum=0.0, nesterov=False)\n",
    "#opt = RMSprop(lr=0.001, rho=0.9)\n",
    "#opt = Adagrad(lr=0.01)\n",
    "#opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "#opt = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "#opt = Adadelta(lr=0.001, rho=0.95)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "history = model.fit(\n",
    "    [trainAttrX, trainImagesX], trainY,\n",
    "    validation_data=([testAttrX, testImagesX], testY),\n",
    "    epochs=100, batch_size=10, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights-improvement-02-0.20.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss during training\n",
    "plt.title('Loss / Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting FIELD ...\")\n",
    "preds = model.predict([testAttrX, testImagesX])\n",
    "pd.DataFrame(preds).to_excel('predictions_combinedNNN.xlsx')\n",
    "\n",
    "preds_trainY = model.predict([trainAttrX, trainImagesX])\n",
    "pd.DataFrame(preds_trainY).to_excel('preds_trainY.xlsx')\n",
    "pd.DataFrame(preds).to_excel('preds_testY.xlsx')\n",
    "\n",
    "pd.DataFrame(trainY).to_excel('real_trainY.xlsx')\n",
    "pd.DataFrame(testY).to_excel('real_testY.xlsx')\n",
    "\n",
    "# compute the difference between the *predicted* field productivities and the\n",
    "# *actual* field productivities, then compute the percentage difference and\n",
    "# the absolute percentage difference\n",
    "diff = preds.flatten() - testY\n",
    "percentDiff = (diff / testY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "# compute the mean and standard deviation of the absolute percentage\n",
    "# difference\n",
    "mean = np.mean(absPercentDiff)\n",
    "std = np.std(absPercentDiff)\n",
    "\n",
    "# finally, show some statistics on our model\n",
    "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
    "print(\"[INFO] avg. FIELD...: {}, std FIELD: {}\".format(locale.currency(df[\"Y1\"].mean(), grouping=True),locale.currency(df[\"Y1\"].std(), grouping=True)))\n",
    "print(\"[INFO] mean: {:.2f}, std: {:.2f}\".format(mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Part (Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = history.history['val_loss'][-1]\n",
    "#lernen = random.choice([0.01, 0.001, 0.0001, 0.00001])\n",
    "#epokhi = int(random.uniform(25, 150))\n",
    "#batsch = int(random.uniform(1, 15))\n",
    "#    \n",
    "#while i > 0.423642161533107:\n",
    "#    gateFactor = Input(tensor = K.variable([0.3]))\n",
    "#    model = Model(inputs=[mlp.input, cnn.input, gateFactor], outputs=x)\n",
    "#    opt = Adam(lr=lernen, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "#    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "#    history = model.fit(\n",
    "#    [trainAttrX, trainImagesX], trainY,\n",
    "#        validation_data=([testAttrX, testImagesX], testY),\n",
    "#        epochs=epokhi, batch_size=batsch)\n",
    "#    i = history.history['val_loss'][-1]\n",
    "#    lernen = random.choice([0.01, 0.001, 0.0001, 0.00001])\n",
    "#    epokhi = int(random.uniform(25, 150))\n",
    "#    batsch = int(random.uniform(1, 15)\n",
    "\n",
    "#if i < 0.423642161533107:\n",
    "#    preds = model.predict([testAttrX, testImagesX])\n",
    "#    pd.DataFrame(preds).to_excel('predictions_combinedNACHT.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os,sys\n",
    "try:\n",
    "    import lime\n",
    "except:\n",
    "    sys.path.append(os.path.join('..', '..')) # add the current directory\n",
    "    import lime\n",
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inet_model = cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the 16th observation for the cheque of the LIME\n",
    "time\n",
    "# Hide color is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels\n",
    "explanation = explainer.explain_instance(images[45], inet_model.predict, top_labels=5, hide_color=0, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just field before the lIME\n",
    "\n",
    "# I'm dividing by 2 and adding 0.5 because of how this Inception represents images\n",
    "plt.imshow(images[45])\n",
    "#preds = inet_model.predict(images)\n",
    "#for x in decode_predictions(preds)[0]:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are several examples of division of the field into sectors.\n",
    "#The number of superpixels is 5.\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_img_fn(path_list):\n",
    "    out = []\n",
    "    for img_path in path_list:\n",
    "        img = image.load_img(img_path, target_size=(128, 64))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = inc_net.preprocess_input(x)\n",
    "        out.append(x)\n",
    "    return np.vstack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grey is for the part which don't have the positive impact on Y_i\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=2, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=1, hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=0, hide_rest=True, min_weight=0.1)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=1, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Дивеевский район, Нижегородская область\n",
    "#For my thesis I would prefer this image. The green part has positive impact on Y_i, the orange one reduces the probability of a high level of the crop. Others don't the impact. \n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=3, hide_rest=False)\n",
    "plot = plt.imshow(mark_boundaries(temp, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Часть изображения, ответственная за весенний снимок, показывает недостаточную растительность для данного времени года,\n",
    "#следоватлеьно, прогнозное значение урожайности для данного наблюдения по МО уменьшается из-за данного участка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a train set of background examples to take an expectation over\n",
    "background = trainImagesX[np.random.choice(trainImagesX.shape[0], 75, replace=False)]\n",
    "\n",
    "# explain predictions of the model on all test images\n",
    "e = shap.DeepExplainer((cnn.layers[4].input, cnn.layers[-1].output), background)\n",
    "shap_values = e.shap_values(testImagesX[3:4])\n",
    "\n",
    "# plot the feature attributions\n",
    "shap.image_plot(shap_values, testImagesX[3:4], labels=None, show=True, width=200, aspect=0.2, hspace=0.2, labelpad=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature attributions\n",
    "shap.image_plot(shap_values, testImagesX[3:4], labels=None, show=True, width=20, aspect=0.5, hspace=0.2, labelpad=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#red pixels are the sectors which positively influence on Y_i\n",
    "#blau pixels have the opposite effect\n",
    "#Выберем 4-е по счету поле для примера.\n",
    "#Для данного изображения (Дивеевский район, Нижегородская область) позитивное воздействие на урожайность\n",
    "#оказывает река в северной части поля в оба сезона года. Негативное влияние на урожайность выявлено у юго-западного \n",
    "#участка с низкой урожайностью весной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Интерпретаторы LIME и SHAP приходят к похожим вывожам, однако для SHAP объяснения более детальны, так как, \n",
    "#в отличие от lIME, здесь нет суперпикселей, искуственно укркупненных секторов изображения, которые \n",
    "#несколько упрощают фогорафию. Также SHAP, согласно теории, является единственным на данный момент методом,\n",
    "#который предоставляет полное, а не локальное объяснение модели глубокого обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Part (Tabular Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mlp.predict([testAttrX])\n",
    "preds = pd.DataFrame(data=preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAttrX=pd.DataFrame(data=trainAttrX)\n",
    "trainAttrX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAttrX=pd.DataFrame(data=testAttrX)\n",
    "testAttrX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(testAttrX.as_matrix(), \n",
    "                                                   feature_names=cols[1:], \n",
    "                                                   class_names=['Y1'], \n",
    "                                                   verbose = True,\n",
    "                                                   mode='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc = testAttrX.as_matrix()[2]\n",
    "qc_reshape = qc.reshape(1,-1)\n",
    "def predict(qc):\n",
    "    global mlp\n",
    "    qc = mlp.predict(qc)\n",
    "    return qc.reshape(qc.shape)\n",
    "exp = explainer.explain_instance(qc, predict, num_features=len(df.columns[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([])\n",
    "variables = ['X1','X2','logX3','X4','X5', 'logX6','X7','X8','X9','X10', 'X11', 'X12', 'X13','X14']\n",
    "for i in range (0, 25):\n",
    "    qc = testAttrX.as_matrix()[2]\n",
    "    qc_reshape = qc.reshape(1,-1)\n",
    "    def predict(qc):\n",
    "        global mlp\n",
    "        qc = mlp.predict(qc)\n",
    "        return qc.reshape(qc.shape[0])\n",
    "    exp = explainer.explain_instance(qc, predict, num_features=len(df.columns[1:]))\n",
    "    #results.append(exp.as_list())\n",
    "    values = []\n",
    "    for var in variables:\n",
    "        for item in exp.as_list():\n",
    "            if var in item[0]:\n",
    "                values.append(item[1])\n",
    "    results[i] = values\n",
    "results.head()\n",
    "results = results.T\n",
    "# results.columns = variables\n",
    "# results.head()\n",
    "#     exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([])\n",
    "variables = [\"X3_dummy\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\", \"X14_dummy\", \"logX15\", \"logX16\", \"X17\", \"X18\", \"X19\", \"logX20\", \"logX21\", \"X22\", \"X23_dummy\", \"X24_dummy\", \"X25\", \"X26\", \"X27\", \"X28\", \"X29\"]\n",
    "for i in range (0, 25):\n",
    "    qc = testAttrX.as_matrix()[2]\n",
    "    qc_reshape = qc.reshape(1,-1)\n",
    "    def predict(qc):\n",
    "        global mlp\n",
    "        qc = mlp.predict(qc)\n",
    "        return qc.reshape(qc.shape)\n",
    "    exp = explainer.explain_instance(qc, predict, num_features=len(df.columns))\n",
    "    #results.append(exp.as_list())\n",
    "    values = []\n",
    "    for var in variables:\n",
    "        for item in exp.as_list():\n",
    "            if var in item[0]:\n",
    "                values.append(item[1])\n",
    "    results[i] = values\n",
    "results.head()\n",
    "results = results.T\n",
    "results.columns = variables\n",
    "results.head()\n",
    "#     exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_excel('results_lime_combined.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAttrX = testAttrX.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(shap.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAttrX_ = pd.DataFrame(testAttrX, columns=[\"X3_dummy\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\", \"X14_dummy\", \"logX15\", \"logX16\", \"X17\", \"X18\", \"X19\", \"logX20\", \"logX21\", \"X22\", \"X23_dummy\", \"X24_dummy\", \"X25\", \"X26\", \"X27\", \"X28\", \"X29\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a set of background examples to take an expectation over \n",
    "background = testAttrX[np.random.choice(testAttrX.shape[0], 25, replace=False)] \n",
    "  \n",
    "# explain predictions of the model on four images \n",
    "#e = shap.GradientExplainer(model, trainAttrX) \n",
    " # ...or pass tensors directly \n",
    "e = shap.DeepExplainer((mlp.layers[0].input, mlp.layers[-1].output), background) \n",
    "shap_values = e.shap_values(testAttrX) \n",
    "  \n",
    " # plot the feature attributions \n",
    "#shap.image_plot(shap_values, trainAttrX) \n",
    "shap.force_plot(e.expected_value[0], shap_values[0][2,:], testAttrX_.iloc[2,:], link=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(e.expected_value[0], shap_values[0], testAttrX_, link=\"identity\") #ДЛя всех наблюдений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = np.array(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, testAttrX_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, testAttrX_, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sv=[]\n",
    "for item in shap_values:\n",
    "    for item_ in item:\n",
    "        print(item_)\n",
    "        a=[]\n",
    "        for i in item_:\n",
    "            print(i)\n",
    "            a.append(i)\n",
    "        sv.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = pd.DataFrame(sv)\n",
    "sv.columns = results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"logX15\", shap_values[0], testAttrX_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], testAttrX_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
